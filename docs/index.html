<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="Scaling Motion Generation Model with  Million-Level Motion Benchmark">
    <meta name="keywords" content="Humanoid Robot, Agent, VLM, Dexterous Manipulation">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Scaling Motion Generation Model with  Million-Level Motion Benchmark</title>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-4J5P615QNV"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-4J5P615QNV');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./css/bulma.min.css">
    <link rel="stylesheet" href="./css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./css/bulma-slider.min.css">
    <link rel="stylesheet" href="./css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./css/index.css">
    <link rel="icon" href="./images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./js/fontawesome.all.min.js"></script>
    <script src="./js/bulma-carousel.min.js"></script>
    <script src="./js/bulma-slider.min.js"></script>
    <script src="./js/index.js"></script>
</head>
<body>


  <section class="hero is-link is-fullheight video" style="overflow: hidden; position:relative;">
    <div class="hero-video" style="height: 100%; width: 177.77777778vh; min-width: 100%;min-height: 56.25vw;">
        <img src="./assets/head.png" alt="Dataset Image" style="width: 100%; height: 100%; object-fit: cover;">
    </div>
    <div class="hero-video is-hidden-tablet is-inline-block-mobile"
        style="height: 154.28571428vw; width: 100%; min-width:64.81481481vh;min-height:100%;">
        <img src="./assets/head.png" alt="Dataset Image" style="width: 100%; height: 100%; object-fit: cover;">
    </div>
    <div class="overlay"></div>

    <div class="hero-head">
      <header class="navbar" style="position: absolute; top: 10px; right: 30px; background: transparent; box-shadow: none;">
            <div class="container is-size-5">
                <!-- <div class="navbar-menu">
                    <div class="navbar-end">
                        <a class="navbar-item pl-4 pr-4" href="https://arxiv.org/pdf/2505.16602">
                            <span class="icon" style="margin-right:5px;">
                                <img src="./images/icon/pdf.svg" alt="PDF" />
                            </span>
                            <span>Paper</span>
                        </a>
                        <a class="navbar-item  pl-4 pr-4" href="https://arxiv.org/abs/2505.16602">
                            <span class="icon" style="margin-right:5px;">
                                <img src="./images/icon/arxiv.svg" alt="arXiv" />
                            </span>
                            <span>arXiv</span>
                        </a>
                        <a class="navbar-item  pl-4 pr-4" href="https://github.com/BeingBeyond/Being-M0">
                            <span class="icon" style="margin-right:5px;">
                                <img src="./images/icon/github.svg" alt="github" />
                            </span>
                            <span>Project</span>
                        </a>
                    </div>
                </div> -->
            </div>
        </header>
        </div>
      
    </section>

<section class="hero">
    <div class="hero-body">
        <div class="container">
            <div class="columns is-centered">
                <div class="column has-text-centered">

                    <h1 class="title is-1 publication-title is-size-4-mobile">MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion Generation</h1>
                    <h1> 
                    <div class="is-size-4 publication-authors is-size-6-mobile">
                    <span class="author-block">
                        <a href="https://zhoubohan0.github.io/">Bohan Zhou</a><sup>&sstarf;</sup>&nbsp;&nbsp;
                    </span>
                    <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=Enhky14AAAAJ">Yi Zhan</a><sup>&sstarf;</sup>&nbsp;&nbsp;
                    </span>
                    <span class="author-block">
                        <a href="">Zhongbin Zhang</a>&nbsp;&nbsp;
                    </span>
                    <span class="author-block">
                      <a href="https://z0ngqing.github.io/">Zongqing Lu</a><sup>&sect;</sup>
                    </span>
                    </div>
                    </h1>
                   
                    <br>
                    <div class="is-size-4 publication-authors is-size-6-mobile">
                        <span class="author-block" style="font-size: 100%"> PKU </span>&nbsp;&nbsp;&nbsp;
                        <span class="author-block" style="font-size: 100%"> THU </span>&nbsp;&nbsp;&nbsp;
                        <span class="author-block" style="font-size: 100%"> BeingBeyond </span>
                    </div>

                    <div class="is-size-5 publication-authors is-size-7-mobile">
                      <span class="author-block"><sup>&sstarf;</sup> Equal contribution</span>
                    </div>
                    <div class="is-size-5 publication-authors is-size-7-mobile">
                        <span class="author-block"><sup>&sect;</sup> Corresponding author</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- PDF Link. -->
                            <span class="link-block">
                <a href="https://arxiv.org/pdf/2505.16602"
                        class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
                            <span class="link-block">
                <a href="https://arxiv.org/abs/2505.16602"
                        class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <!-- Video Links -->
                            <!--              <span class="link-block">-->
                            <!--                <a href="https://www.youtube.com/watch?v=Cx-D708BedY"-->
                            <!--                   class="external-link button is-normal is-rounded is-dark">-->
                            <!--                  <span class="icon">-->
                            <!--                      <i class="fab fa-youtube"></i>-->
                            <!--                  </span>-->
                            <!--                  <span>Video - Main Storyline</span>-->
                            <!--                </a>-->
                            <!--              </span>-->
                            <!--              <span class="link-block">-->
                            <!--                <a href="https://www.youtube.com/watch?v=Oa4Ese8mMD0"-->
                            <!--                   class="external-link button is-normal is-rounded is-dark">-->
                            <!--                  <span class="icon">-->
                            <!--                      <i class="fab fa-youtube"></i>-->
                            <!--                  </span>-->
                            <!--                  <span>Video - Open-ended World</span>-->
                            <!--                </a>-->
                            <!--              </span>-->
                            <!-- Code Link. -->
                            <span class="link-block">
                <a href="https://github.com/BeingBeyond/Being-M0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Project</span>
                  </a>
              </span>
                        </div>
                    </div>

                </div>
            </div>
        </div>
    </div>
</section>



<section class="hero ">
  <!-- is-light is-small -->
  <div class="hero-body">
      <div class="container">
          <div class="three-item">
            <img src="./assets/vis1.jpg" width="50%">
            <img src="./assets/vis2.jpg" width="50%">
          <div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
              <h2 class="title is-3 is-size-4-mobile">Abstract</h2>
              <div class="content has-text-justified">
                  <p>
                    Egocentric hand-object motion generation is crucial for immersive AR/VR and robotic imitation but remains challenging due to unstable viewpoints, self-occlusions, perspective distortion, and noisy ego-motion. Existing methods rely on predefined 3D object priors, limiting generalization to novel objects, which restricts their generalizability to novel objects. Meanwhile, recent multimodal approaches suffer from ambiguous generation from abstract textual cues, intricate pipelines for modeling 3D hand-object correlation, and compounding errors in open-loop prediction. We propose <span class="method" style="font-weight: bold; color:rgb(86, 130, 250);">MEgoHand</span>, a multimodal model that synthesizes physically plausible hand-object interactions from egocentric RGB, text, and initial hand pose. MEgoHand introduces a bi-level architecture: a high-level “cerebrum” leverages a vision language model (VLM) to infer motion priors from visual-textual context and a monocular depth estimator for object-agnostic spatial reasoning, and a low-level DiT-based motion decoder generates fine-grained trajectories via flow-matching, along with temporal orthogonal filtering to enhance to enhance smoothness. To unify heterogeneous datasets, we design Inverse MANO Retargeting Network and Virtual RGB-D Renderer, curating a unified dataset of 3.35M RGB-D frames, 24K interactions, and 1.2K objects. Extensive experiments across 5 in-domain and 2 cross-domain datasets demonstrate the effectiveness of MEgoHand, achieving substantial reductions in wrist translation error (86.9%<span class="method" style="font-weight: bold; color:cornflowerblue;">&#x2B07;</span>) and joint rotation error (34.1%<span class="method" style="font-weight: bold; color:cornflowerblue;">&#x2B07;</span>), highlighting its capacity to accurately model fine-grained hand joint structures and generalize robustly across diverse scenarios.
                  </p>
              </div>
          </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column one-three-thirds">
            <h2 class="title is-3 is-size-4-mobile" style="text-align: center;">Multimodal Egocentric Hand Motion Generator
            </h2>
            <h2>
                <div class="content has-text-justified">
                <span style="font-weight: bold;">[Pipeline]</span> Built upon <span style="font-weight: bold;">Eagle-2</span>, MEgoHand aims to predict a sequence of H-step future MANO parameter sequences. The system prompt and task instruction are encoded using a frozen VLM tokenizer. At each timestep, an RGB image is processed by a pretrained depth estimator to obtain a metric depth map. The RGB and depth images are then combined and encoded into a visual embedding, which—together with the text embedding—is input to the frozen VLM. A DiT-based
                motion generator receives this multimodal representation along with the initial hand parameters to predict relative future hand motion.
                </div> 
            </h2>
            <br><br>
            <img src="./assets/model.png" height="100%">
            <br><br>
            <h2>
              <div class="content has-text-justified">
              <span style="font-weight: bold;">[Decoding Stategy]</span> To ensure temporal coherence in the generated motion sequences, we propose <span style="font-weight: bold;">Temporal Orthogonal Filtering (TOF)</span>, a training-free decoding strategy to denoise
              predicted rotation sequences. At each timestep, we query the motion generator to produce overlapping motion chunks. To suppress high-frequency jitter, a temporal convolution with uniform weights aggregates all rotation and translation estimates corresponding to the same timestep. The resulting convolved rotation is then projected onto the closest valid SO(3) manifold via Singular Value Decomposition (SVD). We can freely adjust the frequency of the query to balance inference speed and generation quality.
              From the figure below, we can see that without smooth decoding, the predicted motion exhibits more fluctuations, which indicates smooth decoding stategy is effective in mitigating jitter.
              </div> 
            </h2>
            <br><br>
            <img 
              src="./assets/TOF.png" 
              style="width: 80%; height: auto;"
            >
        </div>
      </div>
  </div>
</section>  

<section class="section">
  <div class="container">
      <div class="columns is-centered has-text-centered">
      <div class="column one-three-thirds">
          <h2 class="title is-3 is-size-4-mobile" style="text-align: center;">Dataset Curation</h2>
          <h2>
              <div class="content has-text-justified">
              <span style="font-weight: bold;">["Can scaling larger HOI data benefit motion generation?"]</span> MEgohand says "yes"! Existing HOI datasets vary in language
              instructions, annotation quality, and hand pose representations. We systematically integrate and preprocess large-scale public datasets into a unified and standardized training corpus. Two prominent challenges get in the way:
            </h2><br>
          <h2>
            <div class="content has-text-justified">
              1. Some datasets like FPHA only provide 3D hand joint positions captured using wearable sensors instead of MANO parameters. We design <span style="font-weight: bold;">Inverse MANO Retargeting Network</span> which recovers the MANO parameters from raw hand keypoints.
          </h2>
          <br><br>
          <img 
            src="./assets/invmano.png" 
            style="width: 80%; height: auto;"
          >
          <br><br>

          <h2>
            <div class="content has-text-justified">
            2. Many existing datasets, such as ARCTIC, HOT3D, and OakInk2, provide RGB image sequences and object models without corresponding depth maps. We design <span style="font-weight: bold;">Virtual RGB-D Renderer</span> to synthesize depth images aligned with the available RGB frames.
          </h2>
          <br><br>
          <img 
            src="./assets/depth.png" 
            style="width: 60%; height: auto;"
          >
          <br><br>
          <h2>
            <div class="content has-text-justified">
            Finally, we curate a unified multimodal dataset covering <span style="font-weight: bold"> 3.35M RGB-D frames, 24K interaction trajectories, and 1.2K objects</span>, -- 15&times; larger than previous work. 
          </h2>
          <br><br>
          <img 
            src="./assets/datasets.png" 
            style="width: 50%; height: auto;"
          >
          <br><br>
          <h2>
              <div class="content has-text-justified">
              This initiative provides a solid foundation for building robust, universally applicable motion models and offers a comprehensive testbed for future research.
              Training on the curated dataset, we outperform the baseline LatentAct on both in-domain and cross-domain benchmarks.
              </div> 
          </h2>
          <br><br>
          <img 
            src="./assets/bar.png" 
            style="width: 70%; height: auto;"
          >
      </div>
      </div>
  </div>
</section>



<section class="hero is-light is-small">
  <div class="column is-three-thirds">
    <h2 class="title is-3 is-size-4-mobile" style="text-align: center;">Visualizations</h2>
  </div>
  <div class="hero-body">
      <div class="container">
        <div id="results-carousel1" class="carousel results-carousel">

          <div class="item">
            <figure style="margin: 0; text-align: center;">
              <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">[H2O]: Flip a book.</figcaption>
              <video width="100%" height="auto" autoplay muted loop playsinline>
                <source src="./assets/videos/H2O_flip_a_book.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </figure>
          </div>

          <div class="item">
            <figure style="margin: 0; text-align: center;">
              <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">[OAKINK2]: Position the alcohol lamp beneath the tripod.</figcaption>
              <video width="100%" height="auto" autoplay muted loop playsinline>
                <source src="./assets/videos/OAKINK2_place_alcohol_lamp.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </figure>
          </div>

          <div class="item">
            <figure style="margin: 0; text-align: center;">
              <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">[HOI4D]: Move a toy train.</figcaption>
              <video width="100%" height="auto" autoplay muted loop playsinline>
                <source src="./assets/videos/HOI4D_move_a_toy_train.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </figure>
          </div>

          <div class="item">
            <figure style="margin: 0; text-align: center;">
              <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">[HOLO]: Press the screen of a small printer.</figcaption>
              <video width="100%" height="auto" autoplay muted loop playsinline>
                <source src="./assets/videos/HOLO_press_a_small_printer.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </figure>
          </div>

          <div class="item">
            <figure style="margin: 0; text-align: center;">
              <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">[ARCTIC]: Operate a phone.</figcaption>
              <video width="100%" height="auto" autoplay muted loop playsinline>
                <source src="./assets/videos/ARCTIC_operate_a_phone.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </figure>
          </div>

        </div>
      </div>



      <div class="container">
        <div id="results-carousel1" class="carousel results-carousel">
          <div class="item">
            <figure style="margin: 0; text-align: center;">
              <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">[HOT3D]: Pour water from the kettle and leave it aside.</figcaption>
              <video width="100%" height="auto" autoplay muted loop playsinline>
                <source src="./assets/videos/HOT3D_pour_water.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </figure>
          </div>

          <div class="item">
            <figure style="margin: 0; text-align: center;">
              <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">[TACO]: Roll the box with the roller.</figcaption>
              <video width="100%" height="auto" autoplay muted loop playsinline>
                <source src="./assets/videos/TACO_roller_brush_box.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </figure>
          </div>

          <div class="item">
            <figure style="margin: 0; text-align: center;">
              <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">[HOLO]: Sprinkle a sugar packet into the coffee.</figcaption>
              <video width="100%" height="auto" autoplay muted loop playsinline>
                <source src="./assets/videos/HOLO_sprinkle_sugar_into_coffee.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </figure>
          </div>

          <div class="item">
            <figure style="margin: 0; text-align: center;">
              <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">[HOI4D]: Pick and place a toy car.</figcaption>
              <video width="100%" height="auto" autoplay muted loop playsinline>
                <source src="./assets/videos/HOI4D_pick_place_a_toy_car.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </figure>
          </div>

          <div class="item">
            <figure style="margin: 0; text-align: center;">
              <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">[ARCTIC]: Use a capsule machine.</figcaption>
              <video width="100%" height="auto" autoplay muted loop playsinline>
                <source src="./assets/videos/ARCTIC_use_a_capsulemachine.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </figure>
          </div>

        </div>
      </div>
  </div>
</section>

<!--
<section class="hero is-light is-small">
  <div class="hero-body">
      <div class="container">
        <div id="results-carousel1" class="carousel results-carousel">
            <div class="item">
              <figure style="margin: 0; text-align: center;">
                <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">a person is dancing the waltz</figcaption>
                <img src="./images/vis_render_7.gif" height="100%">
              </figure>
            </div>
            <div class="item">
              <figure style="margin: 0; text-align: center;">
                <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">a man quickly turns left using his left heel as the pivot point</figcaption>
                <img src="./images/vis_render_8.gif" height="100%">  
              </figure>
            </div>
            <div class="item">
              <figure style="margin: 0; text-align: center;">
                <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">a person is jumping up and landing</figcaption>
                <img src="./images/vis_render_9.gif" height="100%">
              </figure>
            </div>
            <div class="item">
              <figure style="margin: 0; text-align: center;">
                <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">this person kicks with their right leg</figcaption>
                <img src="./images/vis_render_10.gif" height="100%">
            </figure>
            </div>
            <div class="item">
              <figure style="margin: 0; text-align: center;">
                <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">a man squats extraordinarily low then bolts up</figcaption>
                <img src="./images/vis_render_11.gif" height="100%">
            </figure>
            </div>
            <div class="item">
              <figure style="margin: 0; text-align: center;">
                <figcaption style="margin-top: 0.5rem; font-size: 0.9rem; color: #333;">a woman is doing a ballet</figcaption>
                <img src="./images/vis_render_12.gif" height="100%">
            </figure>
            </div>
            </div>
          <div>
      </div>
  </div>
</section>
-->

<!--
<section class="hero is-light is-small">
  <div class="column is-three-thirds">
    <h2 class="title is-3 is-size-4-mobile" style="text-align: center;">Simulated Applications</h2>
  </div>
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel1" class="carousel results-carousel">
        <div class="item item-a">
          <img id="a" src="./images/sim_1.gif" height="100%">
        </div>
        <div class="item item-b">
          <img id="b" src="./images/sim_2.gif" height="100%">
        </div>
        <div class="item item-c">
          <img id="c" src="./images/sim_3.gif" height="100%">
        </div>
        <div class="item item-d">
          <img id="d" src="./images/sim_4.gif" height="100%">
        </div>
        <div class="item item-e">
          <img id="e" src="./images/sim_5.gif" height="100%">
        </div>
        <div class="item item-f">
          <img id="f" src="./images/sim_6.gif" height="100%">
        </div>
      </div>
    </div>
  </div>
</section>
-->

<!-- <section class="hero is-light is-small">
  <div class="column is-three-thirds">
    <h2 class="title is-3 is-size-4-mobile" style="text-align: center;">Humanoid Robots</h2>
  </div>
  <div class="hero-body">
      <div class="container">
          <div class="six-item">
            <div class="item">
                <img src="./images/sim1-1.gif" height="100%">
            </div>
            <div class="item">
                <img src="./images/sim1-2.gif" height="100%">
            </div>
            <div class="item">
                <img src="./images/sim2-1.gif" height="100%">
            </div>
            <div class="item">
              <img src="./images/sim2-2.gif" height="100%">
            </div>
            <div class="item">
              <img src="./images/sim3-1.gif" height="100%">
            </div>
            <div class="item">
              <img src="./images/sim3-2.gif" height="100%">
            </div>
            <div class="item">
              <img src="./images/sim4-1.gif" height="100%">
            </div>
            <div class="item">
              <img src="./images/sim4-2.gif" height="100%">
            </div>
            <div class="item">
              <img src="./images/sim5-1.gif" height="100%">
            </div>
            <div class="item">
              <img src="./images/sim5-2.gif" height="100%">
            </div>
            <div class="item">
              <img src="./images/sim6-1.gif" height="100%">
            </div>
            <div class="item">
              <img src="./images/sim6-2.gif" height="100%">
            </div>
          <div>
      </div>
  </div>
  <div class="container">
    <div class="columns is-centered has-text-centered">
        <div class="column is-three-thirds">
          <div class="content has-text-justified">
            <br><br>
            <span style="font-weight: bold;">[Animation/Robot Motion]</span>  
              Being-M0 generates motions that can be retargeted to any animated characters and to humanoid robots 
              like Unitree H1, enabling them to perform human-like actions without pre-defined execution.
              There are many potential applications of motion models in both simulated and real worlds. 
              <span class="method" style="font-weight: bold; color:cornflowerblue;">Real-world Demo is on the way</span>.
          </div>
        </div>
    </div>
 </div> 
</section> -->


<!--
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel1" class="carousel results-carousel">
        <div class="item item-a">
          <img id="a" src="./images/sim1-1.gif" width="100%">
        </div>
        <div class="item item-b">
          <img id="b" src="./images/sim1-2.gif" height="100%">
        </div>
        <div class="item item-c">
          <img id="c" src="./images/robot_3.gif" height="100%">
        </div>
        <div class="item item-d">
          <img id="d" src="./images/robot_4.gif" height="100%">
        </div>
        <div class="item item-e">
          <img id="e" src="./images/robot_5.gif" height="100%">
        </div>
        <div class="item item-f">
          <img id="f" src="./images/robot_6.gif" height="100%">
        </div>
      </div>
    </div>
  </div>
  <div class="container">
    <div class="columns is-centered has-text-centered">
        <div class="column is-three-thirds">
          <div class="content has-text-justified">
            <br><br>
            <span style="font-weight: bold;">[Animation/Robot Motion]</span>  
              Being-M0 generates motions that can be retargeted to any animated characters in the simulated world and to robots 
              like Unitree H1, enabling them to perform human-like actions without pre-defined execution while adhering to physical laws.
              There are many potential applications of motion models in both simulated and real worlds. 
          </div>
        </div>
    </div>
 </div> 
</section>
-->
<!--
<section class="section">
  <div class="container">
      <div class="columns is-centered has-text-centered">
          <div class="column is-three-thirds">
              <h2 class="title is-3 is-size-4-mobile" style="text-align: center;">Sim and Real Application</h2>
              <div class="hero-body">
                <div class="container">
                  <div id="results-carousel1" class="carousel results-carousel">
                    <div class="item item-a">
                      <img id="a" src="./images/sim_1.gif"  height="100%">
                    </div>
                    <div class="item item-b">
                      <img id="b" src="./images/sim_2.gif"  height="100%">
                    </div>
                    <div class="item item-c">
                      <img id="c" src="./images/sim_3.gif"  height="100%">
                    </div>
                    <div class="item item-a">
                      <img id="d" src="./images/sim_4.gif"  height="100%">
                    </div>
                    <div class="item item-b">
                      <img id="e" src="./images/sim_5.gif"  height="100%">
                    </div>
                    <div class="item item-c">
                      <img id="f" src="./images/sim_6.gif"  height="100%">
                    </div>
                  </div>
                </div>
              </div>
              <h2>
                  <div class="content has-text-justified">
                      <span style="font-weight: bold;">[Animation Motion Tracking]</span>  
                      Our model allows motion retargeting to any animated character in the simulated world while adhering to physical laws.
                  </div>
              </h2>    
              <div class="hero-body">
                <div class="container">
                  <div id="results-carousel1" class="carousel results-carousel">
                    <div class="item item-a">
                      <img id="a" src="./images/robot_1.gif" height="100%">
                    </div>
                    <div class="item item-b">
                      <img id="b" src="./images/robot_2.gif" height="100%">
                    </div>
                    <div class="item item-c">
                      <img id="c" src="./images/robot_3.gif" height="100%">
                    </div>
                    <div class="item item-d">
                      <img id="d" src="./images/robot_4.gif" height="100%">
                    </div>
                    <div class="item item-e">
                      <img id="e" src="./images/robot_5.gif" height="100%">
                    </div>
                    <div class="item item-e">
                      <img id="e" src="./images/robot_6.gif" height="100%">
                    </div>
                  </div>
                </div>
              </div>
              <h2>
                <div class="content has-text-justified">
                    <span style="font-weight: bold;">[Robot Motion Tracking]</span>  
                    More importantly, the generated motions can be retargeted to robots like H1, enabling them to perform human-like actions without pre-defined execution.
                    In this work, we demonstrate simple robot demos (e.g., hanging in the air). 
                    For more advanced motion tracking—where the robot follows human motions while adhering to physical laws (e.g., maintaining balance) --
                    our latest project <a href="https://github.com/BeingBeyond/Jaeger/blob/main/xxx">Jaeger</a>.
                </div>
            </h2>   
            </section>
          </div>
      </div>
  </div>
</section>
-->


                
<section class="section">
    <div class="container">
        <div class="columns is-centered has-text-centered">
            <div class="column is-two-thirds">
                <h2 class="title is-3 is-size-4-mobile" style="text-align: center;">Conclusion</h2>
                <div class="content has-text-justified">
                    <p>
                      In this paper, we explore how to advance the field of multimodal egocentric HOI motion generation. To this end, We introduce MEgoHand, a multimodal model for egocentric hand motion generation that integrates initial hand parameters, textual instructions, and RGB images to predict realistic hand-object interaction motion sequences. The hierarchical design combines a vision-language model and depth estimation for semantic understanding and 3D reasoning. A DiT-based motion generator conducts closed-loop prediction, enhanced by Temporal Orthogonal Filtering for temporal consistency. To address data scarcity, we curate a million-scale HOI dataset by leveraging inverse MANO retargeting and virtual RGB-D rendering. As an initial attempt to unify vision language models with 3D reasoning for motion generation, MEgoHand demonstrates strong generalization capabilities and achieves SOTA results. 
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section" id="BibTeX">
    <div class="container content">
      <h2 class="title is-3 is-size-4-mobile" style="text-align: center;">BibTeX</h2>
        <pre><code>@article{zhou2025megohand,
  title={MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion Generation},
  author={Zhou, Bohan and Zhan, Yi and Zhang, Zhongbin and Lu, Zongqing},
  journal={arXiv preprint arXiv:2505.16602},
  year={2025}
}
        </code></pre>
    </div>
</section>




<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website template is licensed under a <a rel="license"
                                                                     href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a> and adapted from source at <a
                            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a href="https://umi-on-legs.github.io/">UMI</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>